{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28eb4fb5",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c580938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is  2021-10-20\n"
     ]
    }
   ],
   "source": [
    "# importing the necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print('Today is ', today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cd484",
   "metadata": {},
   "source": [
    "# HTML scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5166198",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Scrape Market Watch\n",
    "def scrape_ticker_mw(ticker):\n",
    "    \n",
    "    url = 'https://www.marketwatch.com/investing/stock/'+ticker.lower()+'?mod=over_search'\n",
    "    r1 = requests.get(url)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')#'html5lib')\n",
    "    coverpage_news = soup1.find_all('div', class_=\"article__content\")\n",
    "    \n",
    "    articles = []\n",
    "    for article in coverpage_news:\n",
    "        try:\n",
    "            title = article.find('h3', class_ = 'article__headline').get_text()\n",
    "            date = (article.find('span', class_ = 'article__timestamp'))['data-est']\n",
    "            author = (article.find('span', class_ ='article__author')).get_text().strip('by ')\n",
    "            articles.append([date.strip(), author.strip(), title.strip()])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "    return articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Scrape Google Finance\n",
    "def scrape_ticker_gf(ticker):\n",
    "    \n",
    "    url = 'https://www.google.com/finance/quote/'+ticker.upper()+':NASDAQ'\n",
    "    r1 = requests.get(url)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')#'html5lib')\n",
    "    coverpage_news = soup1.find_all('div', class_=\"nkXTJ\")\n",
    "    \n",
    "    articles = []\n",
    "    for article in coverpage_news:\n",
    "        #print(article)\n",
    "        try:\n",
    "            title = article.find('div', class_ = 'AoCdqe').get_text().replace('\\n','')\n",
    "            \n",
    "            date = (article.find('div', class_ = 'Adak')).get_text()\n",
    "            date = date.split()\n",
    "            author = (article.find('div', class_ ='sfyJob')).get_text()\n",
    "            if date[1].replace('s','') !='month' and date[1].replace('s','') !='week':\n",
    "                \n",
    "                if date[1].replace('s','') !='day':\n",
    "                    date = today - - timedelta(days=date[0])\n",
    "                    articles.append([date, author, title])\n",
    "                    \n",
    "                elif date[1].replace('s','') !='hour':\n",
    "                    date = today - - timedelta(hours=date[0])\n",
    "                    articles.append([date, author, title])\n",
    "\n",
    "                else:\n",
    "                    print('Strange date')\n",
    "                    print(date)\n",
    "            else:\n",
    "                print('Ignoring this')\n",
    "                print(date)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "        print('\\n')\n",
    "    return articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eb9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Scrape FinViz\n",
    "def scrape_ticker_fv(ticker, verbose = 1, filtering = 0):\n",
    "\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'}\n",
    "    url = 'https://www.finviz.com/quote.ashx?t='+ticker\n",
    "    r1 = requests.get(url, headers=headers)\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html.parser')#'html5lib')\n",
    "    table = soup1.find('table', id='news-table', class_ = 'fullview-news-outer')    #table = soup1.findAll('table')\n",
    "    rows = table.findAll('tr')\n",
    "    \n",
    "    dict_months = {'Jan':'01', 'Feb':'02', 'Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "\n",
    "    day_ = 'none'\n",
    "    articles = []\n",
    "    for article in rows:\n",
    "        #print(article)\n",
    "        try:\n",
    "            title = article.find('a', class_ = 'tab-link-news').get_text()\n",
    "            if filtering != 0 and filtering not in title.lower():\n",
    "                continue\n",
    "                \n",
    "            date = article.find('td').get_text().strip().split()\n",
    "\n",
    "            if len(date)==1:\n",
    "                date = \" \".join([day_, date[0]])\n",
    "            elif len(date) == 2:\n",
    "                day = date[0].split('-')\n",
    "                day_ = \"-\".join(['20'+day[2],dict_months[day[0]], day[1]])\n",
    "                date = \" \".join([day_, date[1]])\n",
    "            else:\n",
    "                print('Date is strange')\n",
    "                print(date)\n",
    "                print(1/0)\n",
    "                \n",
    "            author = article.find('span').get_text()                \n",
    "            articles.append([date, author, title.strip()])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    if verbose == 1:\n",
    "        for article in articles:\n",
    "            print(article)\n",
    "            print('\\n')\n",
    "    return articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aecb9",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa07e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-10-19 10:42PM', ' American City Business Journals', \"The #AppleToo organizer Apple just fired didn't expect to be terminated  but isn't sorry about speaking out\"] \n",
      "\n",
      "['2021-10-19 03:06PM', \" Investor's Business Daily\", 'Dow Jones Gains As Apple Pops; Biden Makes Spending Move; Bitcoin ETF Jumps On Debut'] \n",
      "\n",
      "['2021-10-19 12:32PM', \" Investor's Business Daily Video\", 'Apple Reclaims Key Level'] \n",
      "\n",
      "['2021-10-19 12:20PM', ' Yahoo Finance Video', 'Apple debuts new MacBook Pros with M1 Pro and M1 Max chips'] \n",
      "\n",
      "['2021-10-19 11:39AM', ' Investopedia', 'Apple (AAPL) Is Early Phone Leader in Heated 5G Race'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    articles = scrape_ticker_fv('aapl', verbose = 0, filtering = 'apple')\n",
    "    for a in articles[:5]: print(a, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032651a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
