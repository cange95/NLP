{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT2_Haikus_355M.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W7oQAqQHiFiX",
        "42c_ubVoYoG4",
        "t6nKKhN2Cyj_"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e591e759eb8403da2d0069bc7a4ccdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_422926d868294adcb0cdcfa7ec36766c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99e2fd3dca6944e1a3fa16d6e04ff315",
              "IPY_MODEL_b8f73fc324694929915bb13f3c6f1eb3",
              "IPY_MODEL_ab4abb43e5cf4b9fa0b99eacf88c3a17"
            ]
          }
        },
        "422926d868294adcb0cdcfa7ec36766c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "99e2fd3dca6944e1a3fa16d6e04ff315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ca9d69a682b443c93795c350ac5705f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_713cbc3cd5ab4d9fb30aea2cda4d5e73"
          }
        },
        "b8f73fc324694929915bb13f3c6f1eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_677e4e59c8fc43f7b9c6f873c2f73a48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5234,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5234,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0c46ab9c40b44f695be39cee4e09afb"
          }
        },
        "ab4abb43e5cf4b9fa0b99eacf88c3a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79079878c3e54115838bd1525e327cde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5234/5234 [00:00&lt;00:00, 10700.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_abf1d9e0295f4126bdd906393afdc43d"
          }
        },
        "7ca9d69a682b443c93795c350ac5705f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "713cbc3cd5ab4d9fb30aea2cda4d5e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "677e4e59c8fc43f7b9c6f873c2f73a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0c46ab9c40b44f695be39cee4e09afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79079878c3e54115838bd1525e327cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "abf1d9e0295f4126bdd906393afdc43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c605500c3f4b426b8b71f3b5c7b5f022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f1aa0a3d7656410a89de03e2701b629e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a6e9a1ac613141679b22ece2e5d4fcde",
              "IPY_MODEL_95e188757a754172ae834274e1702ed6",
              "IPY_MODEL_30c57a48a39147589a4d8f2dfacb2351"
            ]
          }
        },
        "f1aa0a3d7656410a89de03e2701b629e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "a6e9a1ac613141679b22ece2e5d4fcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_157ec2ac94534a84a081822e1cae5ddb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Loss: 0.095 — Avg: 0.083 — GPU Mem: 3570 MB:   3%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce176b4645304fb18dbf1539208752e1"
          }
        },
        "95e188757a754172ae834274e1702ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b1440ce9ad8f4ec693734bb43e83a84c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 160,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4eeab079d5424f2895ce2163042a136b"
          }
        },
        "30c57a48a39147589a4d8f2dfacb2351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c494a272b8384f2d976702fc4dd18c05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160/5000 [05:39&lt;2:51:00,  2.12s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b88cb40ff494377b07b9bb5479e8e5f"
          }
        },
        "157ec2ac94534a84a081822e1cae5ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce176b4645304fb18dbf1539208752e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1440ce9ad8f4ec693734bb43e83a84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4eeab079d5424f2895ce2163042a136b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c494a272b8384f2d976702fc4dd18c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b88cb40ff494377b07b9bb5479e8e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cange95/NLP/blob/main/Haikus/GPT2_Haikus_355M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "This notebook is based on the original tutorial from `aitextgen`!\n",
        "\n",
        "- For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "- for `ai-msgbot` (which is using `aitextgen` for chatbot-esque purposes) you can find the project repo [here](https://github.com/pszemraj/ai-msgbot)\n",
        "\n",
        "\n",
        "_updates made by [Peter](https://peterszemraj.ch/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MbTt1SwId0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbtErR7wKxC"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Colaboratory uses a Nvidia K80, an Nvidia P100, an Nvidia V100, or Nvidia A100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a k80 or a P100 is ideal since they have more VRAM. \n",
        "\n",
        "- In theory: **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgMEhF7J3ubw",
        "outputId": "2ffab9f8-cdd0-4110-9cb5-19e64f9f8a62"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 12 12:02:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "### formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "_UJ8ek31Uz-r",
        "outputId": "0f5921c3-d6c7-4433-d5a3-86c1c77900df"
      },
      "source": [
        "# update torch in case using a A100 GPU\n",
        "# may take 5+ minutes\n",
        "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -q"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |██████████████▋                 | 834.1 MB 1.5 MB/s eta 0:10:48tcmalloc: large alloc 1147494400 bytes == 0x5649d87a6000 @  0x7f2181a77615 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed8dd00 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ed1c039 0x56499ed5f409 0x56499ed1ac52 0x56499ed8dc25 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed89915 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.4 MB/s eta 0:08:49tcmalloc: large alloc 1434370048 bytes == 0x564a1cdfc000 @  0x7f2181a77615 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed8dd00 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ed1c039 0x56499ed5f409 0x56499ed1ac52 0x56499ed8dc25 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed89915 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.4 MB/s eta 0:05:55tcmalloc: large alloc 1792966656 bytes == 0x5649a1c2e000 @  0x7f2181a77615 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed8dd00 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ed1c039 0x56499ed5f409 0x56499ed1ac52 0x56499ed8dc25 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed89915 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.3 MB/s eta 0:01:44tcmalloc: large alloc 2241208320 bytes == 0x564a0ca16000 @  0x7f2181a77615 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed8dd00 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ee0cc66 0x56499ed89daf 0x56499ed1c039 0x56499ed5f409 0x56499ed1ac52 0x56499ed8dc25 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed89915 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 1.5 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x564a92378000 @  0x7f2181a761e7 0x56499ed4d067 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee\n",
            "tcmalloc: large alloc 2276827136 bytes == 0x564afec8c000 @  0x7f2181a77615 0x56499ed174cc 0x56499edf747a 0x56499ed1a2ed 0x56499ee0be1d 0x56499ed8de99 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed89c0d 0x56499ed1bafa 0x56499ed89c0d 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1bbda 0x56499ed8a737 0x56499ed889ee 0x56499ed1c271\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 3.3 kB/s \n",
            "\u001b[K     |████████████████████████████████| 24.6 MB 1.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "c1814887-4472-4810-c3c7-9da8e007c2bf"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 572 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 28.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 525 kB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 332 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 34.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 34.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 462 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 30.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 46.2 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3fWgo5DpCYwe",
        "outputId": "797040a3-7847-4145-8af8-f2dd1bb7dbaa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DuC67no59vMe",
        "outputId": "9add571b-8b16-4ee9-98ad-7d80b450ca8b"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "### Loading GPT-2 or GPT Neo\n",
        "\n",
        "\n",
        "- A common use case is *continuing* to fine-tune a model that was originally pretrained, and then fine-tuned a little bit, but needs to be fine-tuned more for accuracy/saliency reasons or because Google cut off the runtime earlier. \n",
        "    - in this case, `load_from_folder` should be set to `True` and `load_folder_dir` points to where the model checkpoint is on your google drive.\n",
        "- **the below section describes loading an new/pretrained model from the original tutorial.**\n",
        "\n",
        "> If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "> There are several sizes of GPT-2:\n",
        "\n",
        "    * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "    * `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "    * `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "> You can also finetune a GPT Neo model instead ([_or any textgen GPT-architecture model on huggingface_](https://huggingface.co/models?pipeline_tag=text-generation)), which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model. (355M parameter model was removed)\n",
        "*  `EleutherAI/gpt-neo-1.3B` : 1.3 billion parameter model. Have yet to see this train on Colab without crashing\n",
        "\n",
        "> The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPsg8XDwHvl"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "c464fcc8-ea5f-4a60-9a1f-8a275d33e697"
      },
      "source": [
        "model_size = \"355M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Language_models/Haikus/GPT2-conversational-355M-Dec-10-2021_t-18\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "flqSlHjMIeIw",
        "outputId": "f5ee3a7a-e43d-4d8a-97e3-812180e47f01"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "# Comment out the above line and uncomment the below line to use GPT Neo instead.\n",
        "# ai = aitextgen(model='distilgpt2', \n",
        "#                to_gpu=True, \n",
        "#                gradient_checkpointing=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/12/2021 12:09:53 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Language_models/Haikus/GPT2-conversational-355M-Dec-10-2021_t-18.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:349: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
            "12/12/2021 12:10:20 — INFO — aitextgen — GPT2 loaded with 354M parameters.\n",
            "12/12/2021 12:10:20 — INFO — aitextgen — Gradient checkpointing enabled for model training.\n",
            "12/12/2021 12:10:20 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "### load training data\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> combine any other data from the a \"standard\" conversational dataset and do a final pass of training (with different data) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "jUWsiVOCUlLo",
        "outputId": "2c0eec70-4388-4a68-dbf4-10cc4c5897de"
      },
      "source": [
        "dl_link = \"https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/wizard-of-wikipedia/ScriptParse-wow-train-kilt.txt\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "hirJkLvbUrMb",
        "outputId": "094e2d75-cc5c-4d68-9d63-699b6bf5471a"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"training_script.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('/content/training_script.txt', <http.client.HTTPMessage at 0x7ff825964290>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEWZnWyYX3y8"
      },
      "source": [
        "# adjust names in script if needed  \n",
        "import pprint as pp\n",
        "from os.path import basename\n",
        "\n",
        "def update_script_names(local_name, spkr_from=\"speaker a\", spkr_to=\"person beta\",\n",
        "                        resp_from=\"speaker b\", resp_to=\"nancy sellers\",\n",
        "                        verbose=False):\n",
        "    \"\"\"\n",
        "    update_script_names - if the textfile script has different names for the \n",
        "    speaker/responder than desired (i.e. it is a group conversation, and the \n",
        "    chatbot is just supposed to simulate 1:1) this function can be used to \n",
        "    standardize\n",
        "    \"\"\"\n",
        "\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        orig_lines = fi.readlines()\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    upd_lines = []\n",
        "\n",
        "    for line in tqdm(orig_lines, total=len(orig_lines), \n",
        "                     desc=\"replacing speaker names\"):\n",
        "        \n",
        "        fixline = line.replace(spkr_from, spkr_to)\n",
        "        fixline = fixline.replace(resp_from, resp_to)\n",
        "        upd_lines.append(fixline)\n",
        "\n",
        "    local_namev2 = join(vm_wd, \"V2-rename-\" + basename(local_name))\n",
        "\n",
        "    with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "        fo.writelines(upd_lines)\n",
        "\n",
        "    if verbose: pp.pprint(upd_lines[:10])\n",
        "    # return filepath\n",
        "    return local_namev2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "6OFnPCLADfll",
        "outputId": "2ef8ff2a-d08c-41b8-8fae-e3304227e1c9"
      },
      "source": [
        "file_name = local_name # update if using fn above\n",
        "print(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/training_script.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load my training data"
      ],
      "metadata": {
        "id": "GJI1W0PAUUV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/drive/MyDrive/Language_models/Haikus/dataset.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "txegZxT-UXQi",
        "outputId": "1232a50d-9900-473f-c3fb-9f2c35d6bc70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_from_gdrive(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "XQ07ja3OUx4G",
        "outputId": "97fa1837-2f0d-44b3-c8d2-2230bc693ae7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "SameFileError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSameFileError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9a43db37b55f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy_file_from_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aitextgen/colab.py\u001b[0m in \u001b[0;36mcopy_file_from_gdrive\u001b[0;34m(file_path, from_folder)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \"\"\"\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSameFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{!r} and {!r} are the same file\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSameFileError\u001b[0m: '/content/drive/MyDrive/Language_models/Haikus/dataset.csv' and '/content/drive/MyDrive/Language_models/Haikus/dataset.csv' are the same file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c_ubVoYoG4"
      },
      "source": [
        "### Load Conv_AI_2 database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Iz3ImyXGChnN",
        "outputId": "f45df03e-70ab-4658-f66a-e4d3f192200b"
      },
      "source": [
        "!pip3 install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 61 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 71 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 81 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 92 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 122 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 133 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 143 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 153 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 163 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 174 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 184 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 194 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 204 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 215 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 225 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 235 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 245 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 256 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 266 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 276 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 286 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 296 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 298 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, datasets\n",
            "Successfully installed datasets-1.16.1 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "PSUDVLexEn1g",
        "outputId": "c3a43303-355e-4119-c036-ec6a5576938f"
      },
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "configs = get_dataset_config_names(\"conv_ai_2\")\n",
        "print(configs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conv_ai_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "PbKkkkALCx_i",
        "outputId": "6ccb8497-0cc0-4473-e5c6-d5b19ead02c5"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('conv_ai_2', 'conv_ai_2', split='train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/08/2021 12:46:10 — WARNING — datasets.builder — Reusing dataset conv_ai_2 (/root/.cache/huggingface/datasets/conv_ai_2/conv_ai_2/1.0.0/11d600ddce66bb9d07ca50d1b55b488145ef0d5d0206168c32f1043677875865)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "GgzfNdrKEdQQ",
        "outputId": "903d33f0-c534-450e-ff60-7a65fc3002e0"
      },
      "source": [
        "print(dataset)\n",
        "dataset.column_names\n",
        "#dataset.info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'dialog_id', 'dialog', 'bot_profile', 'user_profile', 'eval_score', 'profile_match'],\n",
            "    num_rows: 3495\n",
            "})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'dialog_id',\n",
              " 'dialog',\n",
              " 'bot_profile',\n",
              " 'user_profile',\n",
              " 'eval_score',\n",
              " 'profile_match']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "WpIb1MBoGX2P",
        "outputId": "d2de7886-21ff-4e51-fc12-af476da887b6"
      },
      "source": [
        "dataset['dialog'][11:15]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'id': 0,\n",
              "   'sender': 'participant1',\n",
              "   'sender_class': 'Human',\n",
              "   'text': 'Неllo'}],\n",
              " [{'id': 0,\n",
              "   'sender': 'participant1',\n",
              "   'sender_class': 'Human',\n",
              "   'text': 'Hello'},\n",
              "  {'id': 1,\n",
              "   'sender': 'participant2',\n",
              "   'sender_class': 'Bot',\n",
              "   'text': 'Hello, how are you?'}],\n",
              " [{'id': 0,\n",
              "   'sender': 'participant1',\n",
              "   'sender_class': 'Human',\n",
              "   'text': 'what is your name ?'}],\n",
              " [{'id': 0,\n",
              "   'sender': 'participant1',\n",
              "   'sender_class': 'Human',\n",
              "   'text': 'hello'},\n",
              "  {'id': 1,\n",
              "   'sender': 'participant1',\n",
              "   'sender_class': 'Human',\n",
              "   'text': 'anybody there'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6nKKhN2Cyj_"
      },
      "source": [
        "### Load Persona-Chat database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "MjK1WiXllFxw",
        "outputId": "76d92a56-9864-46ae-a24e-7ef5730a93a4"
      },
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    data = json.loads(response.read().decode('utf8'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "GXYqv0wnlPWe",
        "outputId": "bc75b9e2-69ca-4c67-f6ba-c1008d295008"
      },
      "source": [
        "#data['train'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "CmWj0AUcYnOP",
        "outputId": "890ea23a-081f-4c19-e65c-992a9da8f4e6"
      },
      "source": [
        "# Tokenize and encode the dataset using our loaded GPT tokenizer\n",
        "def tokenize(obj):\n",
        "    if isinstance(obj, str):\n",
        "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "    if isinstance(obj, dict):\n",
        "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "    return list(tokenize(o) for o in obj)\n",
        " \n",
        "dataset = tokenize(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7dcbbf66e725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from pytorch_pretrained_bert import cached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.6.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordpieceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIGPTTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization_transfo_xl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTransfoXLTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransfoXLCorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/boto3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_warn_deprecated_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/boto3/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownServiceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNSIGNED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigprovider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigValueStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/credentials.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtotal_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat_shell_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnknownCredentialError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartialCredentialsError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_POOL_CONNECTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvalidS3AddressingStyleError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvalidRetryConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvendored\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawsrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_request_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPClientError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttpsession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mURLLib3Session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnectionpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPSConnectionPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m from botocore.compat import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawsrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttpsession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from botocore.compat import (\n\u001b[1;32m     34\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_longest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlunsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoolManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_from_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from urllib3.util.ssl_ import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOP_NO_SSLv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOP_NO_SSLv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOP_NO_COMPRESSION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mPROTOCOL_TLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_CIPHERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PROTOCOL_TLS' from 'urllib3.util.ssl_' (/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "REDm2euJ1kHB",
        "outputId": "712392ed-4880-4c6b-9a3f-1c5b2d55b58e"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Language_models/Haikus/\" #@param {type:\"string\"}\n",
        "# update to yours"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sDmyL2A7uhaL",
        "outputId": "13eae8ca-6399-47d3-e394-dc15fbedb4d4"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "from datetime import datetime\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "temp_gpu_path = join(base_dir, \n",
        "                     \"GPT2-conversational-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                            dt=get_timestamp(),\n",
        "                                                            )\n",
        "                     )\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "398"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDXk_iGn-wF"
      },
      "source": [
        "### example outputs\n",
        "\n",
        "- this cell had its outputs cleared before notebook was posted.\n",
        "- the below is an example of what the outputs during training should look like.\n",
        "\n",
        "```\n",
        "1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "1,500 steps reached: generating sample texts.\n",
        "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
        "==========\n",
        ".\n",
        "\n",
        "person alpha:\n",
        "i love to read. reading. reading requires creativity. it's one of the intricacies of decoding symbols and improving knowledge, sharing information through wires and wires.\n",
        "is reading still a good thing to read?\n",
        "not really. reading software allows creativity, refinement and development. it's also a great tool for sharing information and sharing information over the past.\n",
        "can you give example of reading strategies?\n",
        "\n",
        "person beta:\n",
        "its is. reading produces creativity. it's my favorite form of reading, it's used on high shelfs.\n",
        "\n",
        "person alpha:\n",
        "do you like to eat at a restaurant? i'm a big fan of cooking and watching my favorite meal.\n",
        "i love cooking, it is the combination of grilling over an open fire.\n",
        "i love cooking, it is so good! it's my favorite meal of the day. i like to add to my food in my house.\n",
        "it is a great meal too, i like to add many different ways to cooking.\n",
        "i like to add different ways to cooking.\n",
        "i agree, it is the most common type of cooking.\n",
        "\n",
        "person beta:\n",
        "i like to add different techniques and ingredients and ingredients.\n",
        "\n",
        "person alpha:\n",
        "\n",
        "==========\n",
        "2,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "3,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "3,000 steps reached: generating sample texts.\n",
        "==========\n",
        "\n",
        "\n",
        "person alpha:\n",
        "i love to cook. cooking is a great art. you can bake!\n",
        "i love cooking too. cooking as a whole is an art.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1pdV5lovKg"
      },
      "source": [
        "### t r a i n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506,
          "referenced_widgets": [
            "0e591e759eb8403da2d0069bc7a4ccdf",
            "422926d868294adcb0cdcfa7ec36766c",
            "99e2fd3dca6944e1a3fa16d6e04ff315",
            "b8f73fc324694929915bb13f3c6f1eb3",
            "ab4abb43e5cf4b9fa0b99eacf88c3a17",
            "7ca9d69a682b443c93795c350ac5705f",
            "713cbc3cd5ab4d9fb30aea2cda4d5e73",
            "677e4e59c8fc43f7b9c6f873c2f73a48",
            "e0c46ab9c40b44f695be39cee4e09afb",
            "79079878c3e54115838bd1525e327cde",
            "abf1d9e0295f4126bdd906393afdc43d",
            "c605500c3f4b426b8b71f3b5c7b5f022",
            "f1aa0a3d7656410a89de03e2701b629e",
            "a6e9a1ac613141679b22ece2e5d4fcde",
            "95e188757a754172ae834274e1702ed6",
            "30c57a48a39147589a4d8f2dfacb2351",
            "157ec2ac94534a84a081822e1cae5ddb",
            "ce176b4645304fb18dbf1539208752e1",
            "b1440ce9ad8f4ec693734bb43e83a84c",
            "4eeab079d5424f2895ce2163042a136b",
            "c494a272b8384f2d976702fc4dd18c05",
            "3b88cb40ff494377b07b9bb5479e8e5f"
          ]
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "44d7d5a8-ec3e-48ca-84c5-109c244ff0ad"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "\n",
        "ai.train(file_name,\n",
        "         output_dir=temp_gpu_path, # where it saves during \"save_every\"\n",
        "         line_by_line=True, # if using CSV file input\n",
        "         from_cache=False,\n",
        "         num_steps=5000, # takes about 5 hours on 16 gb v100 GPU for 75000\n",
        "         generate_every=250,\n",
        "         max_grad_norm=0.5,\n",
        "         save_every=500,\n",
        "         gradient_accumulation_steps=1,\n",
        "         save_gdrive=True, # this is an \"automated\" save which is worse than current method (IMO)\n",
        "         learning_rate=1e-3,\n",
        "        #  fp16=True, # may be relevant to set to false (even if available) for \"final\" training\n",
        "         batch_size=1, # if pushing model_size you probably want to leave this at 1\n",
        "        #  fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "        #  amp_backend='apex'\n",
        "         freeze_layers = True,\n",
        "         num_layers_freeze = 22\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/12/2021 12:10:57 — INFO — aitextgen — Loading text from /content/drive/MyDrive/Language_models/Haikus/dataset.csv with generation length of 1024.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e591e759eb8403da2d0069bc7a4ccdf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5234 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/12/2021 12:10:58 — INFO — aitextgen.TokenDataset — Encoding 5,234 rows from /content/drive/MyDrive/Language_models/Haikus/dataset.csv.\n",
            "12/12/2021 12:10:58 — INFO — aitextgen — Layer freezing enabled for model training.\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
            "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:168: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
            "  \"Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed\"\n",
            "12/12/2021 12:10:58 — INFO — pytorch_lightning.utilities.distributed — GPU available: True, used: True\n",
            "12/12/2021 12:10:58 — INFO — pytorch_lightning.utilities.distributed — TPU available: False, using: 0 TPU cores\n",
            "12/12/2021 12:10:58 — INFO — pytorch_lightning.utilities.distributed — IPU available: False, using: 0 IPUs\n",
            "12/12/2021 12:10:58 — INFO — pytorch_lightning.accelerators.gpu — LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c605500c3f4b426b8b71f3b5c7b5f022",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1820: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
            "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b223a3fa-1281-4a5b-e428-c3ec3dbd3dc3"
      },
      "source": [
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        "                     )\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a763b546-74b8-45bb-f574-bcb349512c93"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n",
        "\n",
        "print(f'saved! {get_timestamp()}')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved! Dec-10-2021_t-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aitextgen.colab import  copy_file_to_gdrive\n",
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        ")\n",
        "print(save_path)\n",
        "copy_file_to_gdrive(save_path+\"/config.json\")\n",
        "copy_file_to_gdrive(save_path+\"/pythorch_model.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "1gBNkmjAHxIt",
        "outputId": "b2fb5e44-34d3-42c0-d650-f25c423be7cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Language_models/Haikus/FINAL-GPT2-conv-355M-Dec-10-2021_t-20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SameFileError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSameFileError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e5287f34e8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcopy_file_to_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcopy_file_to_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pythorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aitextgen/colab.py\u001b[0m in \u001b[0;36mcopy_file_to_gdrive\u001b[0;34m(file_path, to_folder)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \"\"\"\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSameFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{!r} and {!r} are the same file\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSameFileError\u001b[0m: '/content/drive/MyDrive/Language_models/Haikus/FINAL-GPT2-conv-355M-Dec-10-2021_t-20/config.json' and '/content/drive/MyDrive/Language_models/Haikus/FINAL-GPT2-conv-355M-Dec-10-2021_t-20/config.json' are the same file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi9g2G4hK5y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Use a Trained Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ee26c4a0-553b-400c-8bb3-5d07644d0616"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "#from_folder = save_path\n",
        "from_folder = \"/content/drive/MyDrive/FINAL-GPT2-conv-355M-Dec-09-2021_t-12\"\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/09/2021 12:24:28 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/FINAL-GPT2-conv-355M-Dec-09-2021_t-12.\n",
            "12/09/2021 12:24:36 — INFO — aitextgen — GPT2 loaded with 354M parameters.\n",
            "12/09/2021 12:24:36 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "### Generate Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "4RNY6RBI9LmL",
        "outputId": "add3a7d0-2c49-4e41-ddc7-eb4cb8137b56"
      },
      "source": [
        "ai.generate(n=10, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the old elm\n",
            "==========\n",
            "a snowman\n",
            "on the wire\n",
            "==========\n",
            "my wife\n",
            "and I\n",
            "blood brothers\n",
            "==========\n",
            "moss covered home --\n",
            "an ancient path\n",
            "in a world without a tree\n",
            "==========\n",
            "--\n",
            "mum's wet night\n",
            "her long legs\n",
            "overgrown with snow\n",
            "==========\n",
            "of rain -\n",
            "the stone in her shoe\n",
            "\n",
            "==========\n",
            "'s\n",
            "waiting on the bench\n",
            "her wait\n",
            "==========\n",
            "the moon-\n",
            "as if\n",
            "summer had arrived \n",
            "==========\n",
            "--\n",
            "sundown\n",
            "a long line to buy\n",
            "more bagels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fe1a0d68-3c8d-4e86-fc54-45d4192fa020"
      },
      "source": [
        "ai.generate(prompt=\"['sci-fi' 'horror']-Person 1: Did you just see that?\", temperature=1.0,\n",
        "            min_length=10, batch_size = 1, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m['sci-fi' 'horror']-Person 1: Did you just see that?\u001b[0m\n",
            "['sci-fi' 'horror']-Person 2: Yes.\n",
            "['sci-fi' 'horror']-Person 1: That's the signal you're looking for.\n",
            "['sci-fi' 'horror']-Person 2: Where is it?\n",
            "['sci-fi' 'horror']-Person 1: We've been watching everything unfold!\n",
            "['sci-fi' 'horror']-Person 2: Not for long. We have to find out what is happening.\n",
            "['sci-fi' 'horror']-Person 1: What's wrong? We can't see him.\n",
            "['sci-fi' 'horror']-Person 2: Well he won't be back. We'll find him.\n",
            "['sci-fi' 'horror']-Person 1: The system is destroyed.\n",
            "['sci-fi' 'horror']-Person 2: But what are you going to do now?  It must be hard for you.\n",
            "['sci-fi' 'horror']-Person 1: We're going to destroy the world.\n",
            "['sci-fi' 'horror']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = join(base_dir, \"generated_text_out\")\n",
        "\n",
        "os.makedirs(save_loc, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"person alpha:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "          [\"person alpha:\"+\"\\n\", \n",
        "           \"hello there!\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "           [\"person alpha:\"+\"\\n\", \"why does it always seem that \"],\n",
        "           [\"person beta:\" + \"\\n\"],\n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-dailydialogue-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDdXrPOv9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}