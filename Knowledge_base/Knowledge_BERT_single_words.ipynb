{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59005b62",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35b52a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82a35d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-large-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate \n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ff89e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Others\n",
    "from ipynb.fs.full.SQL_wikidata import find_Qid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cc91f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f574a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Inspiration from\n",
    "####https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
    "###https://github.com/arushiprakash/MachineLearning/blob/main/BERT%20Word%20Embeddings.ipynb\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5fca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer ( out of 12, summing on last 2)\n",
    "    \n",
    "    token_embeddings = (hidden_states[-1] + hidden_states[-2])/2# + hidden_states[-3] + hidden_states[-4]\n",
    "    #print(token_embeddings.shape)\n",
    "\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e6ff53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Recompose words that were splitted in multiple tokens for searching purpose.\n",
    "def recompose_tokens(tokenized_text_, list_token_embeddings_ = None):\n",
    "\n",
    "    if list_token_embeddings_ == None:\n",
    "        tokenized_text = []\n",
    "        \n",
    "        i = 0\n",
    "        while True:\n",
    "\n",
    "            t = tokenized_text_[i]\n",
    "\n",
    "            if '##' not in t: tokenized_text.append(t) \n",
    "\n",
    "            else: tokenized_text[-1] = str(tokenized_text[-1]) + str(t).replace('##','')\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i == len(tokenized_text_): return  tokenized_text\n",
    "\n",
    "    else:\n",
    "        \n",
    "        tokenized_text = []\n",
    "        list_token_embeddings = []\n",
    "        i = 0\n",
    "        c = 1\n",
    "        while True:\n",
    "\n",
    "            t = tokenized_text_[i]\n",
    "            e = list_token_embeddings_[i]\n",
    "            \n",
    "            if '##' not in t:\n",
    "                ###Normalize recomposed element when the recomposition is over\n",
    "                if c > 1:\n",
    "                    list_token_embeddings[-1] = [x/c for x in list_token_embeddings[-1]]\n",
    "                    c = 1\n",
    "\n",
    "                tokenized_text.append(t) \n",
    "                list_token_embeddings.append(e)\n",
    "\n",
    "            else:\n",
    "\n",
    "                tokenized_text[-1] = str(tokenized_text[-1]) + str(t).replace('##','')\n",
    "                list_token_embeddings[-1] = [float(x_) + float(e_) for x_, e_ in zip(list_token_embeddings[-1], e)]\n",
    "                c += 1\n",
    "\n",
    "            i += 1\n",
    "            if i == len(tokenized_text_):\n",
    "\n",
    "                return  tokenized_text, list_token_embeddings\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffbe98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Return position of the search object inside the text\n",
    "def find_s_object(s_object, tokenized_text):\n",
    "\n",
    "    s_object = tokenizer.tokenize(s_object)\n",
    "    s_object = recompose_tokens(s_object)\n",
    "    s_object = [s.lower() for s in s_object]\n",
    "\n",
    "    if len(s_object) == 1:\n",
    "        return [tokenized_text.index(s_object[0])]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        indices = sublist_indices(s_object, tokenized_text)\n",
    "        return indices\n",
    "        \n",
    "    return\n",
    "\n",
    "def sublist_indices(sub, lst):\n",
    "    \n",
    "    ln = len(sub)\n",
    "    for i in range(len(lst) - ln + 1):\n",
    "\n",
    "        if all(sub[j] == lst[i+j] for j in range(ln)):\n",
    "            return [ind for ind in range(i, i + ln)]\n",
    "        \n",
    "    return [np.nan for ind in range(i, i + ln)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0f8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Search entity s_object inside sentence, and understand what it refers to based on context\n",
    "def contextual_entity(s_object, news, unlemmatized_s_object = None, verbose = 0):\n",
    "    \n",
    "    if unlemmatized_s_object == None:\n",
    "        unlemmatized_s_object = s_object\n",
    "        \n",
    "    ###Possible descriptions\n",
    "    s_results = find_Qid_search(s_object)    \n",
    "    snippets = [snippet for snippet in s_results[\"snippet\"] if \"Wikimedia disambiguation page\" not in snippet]# and len(snippet.replace('the App Store is a ', '')) > 0]\n",
    "\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        display(s_results)\n",
    "\"\"\"\n",
    "    # Getting embeddings for the target\n",
    "    # word in all given search results contexts\n",
    "    snip_word_embeddings = []\n",
    "\n",
    "    ###Find embedding of s_object in news\n",
    "    tokenized_text_, tokens_tensor, segments_tensors = bert_text_preparation(news, tokenizer)\n",
    "    list_token_embeddings_ = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "    ###Need to recompose tokens that were decomposed in subtokens:\n",
    "    tokenized_text, list_token_embeddings = recompose_tokens(tokenized_text_, list_token_embeddings_)\n",
    "\n",
    "    # Find the position of s_object in list of tokens\n",
    "    word_index = find_s_object(unlemmatized_s_object, tokenized_text)\n",
    "    \n",
    "    #Embedding for the target word in news (could be more than one)\n",
    "    news_word_embedding = [sum(x)/len(word_index) for x in zip(*list_token_embeddings[word_index[0]:word_index[-1]+1])]\n",
    "\n",
    "\n",
    "\n",
    "    ###Find embeddings of s_object in search results\n",
    "    for res in snippets:\n",
    "\n",
    "        tokenized_text_, tokens_tensor, segments_tensors = bert_text_preparation(res, tokenizer)\n",
    "        list_token_embeddings_ = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "        ###Need to recompose tokens that were decomposed in subtokens:\n",
    "        tokenized_text, list_token_embeddings = recompose_tokens(tokenized_text_, list_token_embeddings_)\n",
    "        \n",
    "        # Find the position of s_object in list of tokens    \n",
    "        #word_index = find_s_object(unlemmatized_s_object, tokenized_text)#tokenized_text.index(s_object.lower())\n",
    "        word_index = find_s_object(s_object, tokenized_text)#tokenized_text.index(s_object.lower())\n",
    "    \n",
    "        # Embedding for the target word in snippets\n",
    "\n",
    "\n",
    "        word_embedding = [sum(x)/len(word_index) for x in zip(*list_token_embeddings[word_index[0]:word_index[-1]+1])]\n",
    "\n",
    "        snip_word_embeddings.append(word_embedding)\n",
    "\n",
    "    list_of_distances = []\n",
    "    for snippet, embed_s in zip(snippets, snip_word_embeddings):\n",
    "\n",
    "        cos_dist = 1 - cosine(embed_s, news_word_embedding)\n",
    "        list_of_distances.append([snippet, cos_dist])\n",
    "\n",
    "    distances_df = pd.DataFrame(list_of_distances, columns=['snippet', 'distance'])\n",
    "    \n",
    "    if verbose == 1:\n",
    "        indices = np.argsort(distances_df[\"distance\"].values)[::-1]\n",
    "        for ind in indices:\n",
    "            print(distances_df.iloc[ind][\"snippet\"], distances_df.iloc[ind][\"distance\"])\n",
    "        print(\"\")    \n",
    "    return distances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb31c6",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63e1a1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_Qid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2684f121fa4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Apple Unleashes New MacBooks , AirPods, Low-Cost Apple Music Plan.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdistances_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextual_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0munlemmatized_s_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MacBooks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#display(distances_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e36882440ec9>\u001b[0m in \u001b[0;36mcontextual_entity\u001b[0;34m(s_object, news, unlemmatized_s_object, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m###Possible descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ms_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_Qid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msnippets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msnippet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msnippet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snippet\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Wikimedia disambiguation page\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnippet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# and len(snippet.replace('the App Store is a ', '')) > 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_Qid_search' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    s_object = 'MacBook'\n",
    "    news = \"Apple Unleashes New MacBooks , AirPods, Low-Cost Apple Music Plan.\"\n",
    "\n",
    "    distances_df = contextual_entity(s_object, news,  unlemmatized_s_object = 'MacBooks')\n",
    "    #display(distances_df)\n",
    "    indices = np.argsort(distances_df[\"distance\"].values)[::-1]\n",
    "    for ind in indices:\n",
    "        print(distances_df.iloc[ind][\"snippet\"], distances_df.iloc[ind][\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b086aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
